{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd023253deaa44fc8808375d0438997a72d943f96b17c9c832de56565a7ac5f0797",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1e725c21e370df5741fa1d5475ea38ef295287de6e4e516b6f2ff7d5eb8025e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------\")\n",
    "print(\"Program Started for DNN Char Yield Model\")\n",
    "print(\"------------------\\n\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics, model_selection, neighbors, svm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "source": [
    "# Loading Data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_p = 'C:\\\\Users\\\\Honeyz\\\\Desktop\\\\Modeling\\\\Final models\\\\CoPyro Project\\\\Data\\\\CharCoPyroDataProcessed_Proximate.csv'\n",
    "\n",
    "raw_dataset = pd.read_csv(dir_p, skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "print(\"--------Data have been loaded & splitted---------\\n\")\n"
   ]
  },
  {
   "source": [
    "# Pearson Corr"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsoncorr = dataset.corr(method='pearson')\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "mask = np.zeros_like(pearsoncorr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]= True\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(pearsoncorr,\n",
    "                      square = True,\n",
    "                      mask=mask,\n",
    "                      linewidths = .5,\n",
    "                      cmap = 'coolwarm',\n",
    "                      cbar_kws = {'shrink': .4,\n",
    "                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n",
    "                      vmin = -1,\n",
    "                      vmax = 1,\n",
    "                      annot = True,\n",
    "                      annot_kws = {'size': 12})\n",
    "\n",
    "#add the column names as labels\n",
    "ax.set_yticklabels(pearsoncorr.columns, rotation = 0)\n",
    "ax.set_xticklabels(pearsoncorr.columns)\n",
    "\n",
    "sns.set_style({'xtick.bottom': True}, {'ytick.left': True})\n"
   ]
  },
  {
   "source": [
    "#### Peason Correlation only capture linear dependancies. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Split and stratification "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = 42 \n",
    "_random_state = np.random.RandomState(_seed)\n",
    "strata_1 = pd.cut(dataset.loc[:, \"w%P\"], bins=[-1, 15, 35, 50, 75, 90, np.inf],labels=[1, 2, 3, 4, 5, 6])\n",
    "training_data, testing_data = model_selection.train_test_split(dataset, test_size=0.1, stratify=strata_1, random_state=_random_state)\n",
    "print(\"--------Data have been stratified----------\\n\")\n"
   ]
  },
  {
   "source": [
    "#### Data is stratified with polymer weight percent in the sample to have samiliar distribution in the test and train samples. This prevents the random split from accidenatly allocate test data out of the training range. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = (testing_data.loc[:, \"w%P\"]\n",
    "                 .hist())\n",
    "plt.show()\n",
    "__ = (dataset.loc[:, \"w%P\"]\n",
    "               .hist())"
   ]
  },
  {
   "source": [
    "# Data preprocessing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = training_data.drop(\"Char%\", axis=1)\n",
    "training_target = training_data.loc[:, [\"Char%\"]]\n",
    "\n",
    "testing_features = testing_data.drop(\"Char%\", axis=1)\n",
    "testing_target = testing_data.loc[:, [\"Char%\"]]\n",
    "print(\"--------Data have been divided into features and targets----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc = StandardScaler()\n",
    "mmc = MinMaxScaler()\n",
    "X_train = mmc.fit_transform(training_features)\n",
    "X_test = mmc.fit_transform(testing_features)"
   ]
  },
  {
   "source": [
    "#### Scaling the data helps the model converge more better"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Training and Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simple random forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(training_features, training_target)\n",
    "\n",
    "predictions = regressor.predict(testing_features)\n",
    "mae = metrics.mean_absolute_error(testing_target, predictions)\n",
    "mse = metrics.mean_squared_error(testing_target, predictions)\n",
    "rmse = mse**0.5\n",
    "\n",
    "print(rmse)\n",
    "print(mae)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Cross Validation with Random Forsest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_regression_scores = model_selection.cross_val_score(RandomForestRegressor(),\n",
    "                                                           X=training_features,\n",
    "                                                           y=training_target,\n",
    "                                                           cv=10,\n",
    "                                                           scoring=\"neg_mean_absolute_error\",\n",
    "                                                           n_jobs=-1)\n",
    "def display_mae(rmses):\n",
    "    print(\"MAE mean:\", rmses.mean())\n",
    "    print(\"MAE standard deviation:\", rmses.std())\n",
    "\n",
    "RandomForest_regression_mae = (-RandomForest_regression_scores)\n",
    "display_mae(RandomForest_regression_mae)"
   ]
  },
  {
   "source": [
    "## Cross Validation with K-nearest nieghbor model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = model_selection.cross_val_score(neighbors.KNeighborsRegressor(),\n",
    "                                             X=training_features,\n",
    "                                             y=training_target,\n",
    "                                             cv=10,\n",
    "                                             scoring=\"neg_mean_absolute_error\",\n",
    "                                             n_jobs=-1)\n",
    "\n",
    "knn_mae = (-knn_scores)\n",
    "display_mae(knn_mae)"
   ]
  },
  {
   "source": [
    "## Cross Validation with Support Vector Machine model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scores = model_selection.cross_val_score(svm.SVR(),\n",
    "                                             X=training_features,\n",
    "                                             y=training_target,\n",
    "                                             cv=10,\n",
    "                                             scoring=\"neg_mean_absolute_error\",\n",
    "                                             n_jobs=-1)\n",
    "\n",
    "svr_mae = (-svr_scores)\n",
    "display_mae(svr_mae)"
   ]
  },
  {
   "source": [
    "# Training and hyperparamers search with Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## XGBoost with Random search to identify the scale of best hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "_param_distributions = {\n",
    "    \"n_estimators\": stats.geom(p=0.01),\n",
    "    \"min_samples_split\": stats.beta(a=1, b=99),\n",
    "    \"min_samples_leaf\": stats.beta(a=1, b=999),\n",
    "}\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 4000, stop = 10000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(\"--------Model and search space have been defined----------\\n\")\n",
    "_random_state = 42\n",
    "xgb_regressor = XGBRegressor(random_state=_random_state)\n",
    "\n",
    "randomized_search_cv = model_selection.RandomizedSearchCV(\n",
    "    xgb_regressor,\n",
    "    param_distributions=_param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    random_state=42,\n",
    "    n_iter=10,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "randomized_search_cv.fit(training_features, training_target)\n",
    "best_random = randomized_search_cv.best_estimator_\n",
    "rsbs = randomized_search_cv.best_params_\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "\n",
    "print(rsbs)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# RMSE for the best parameters\n",
    "print(-randomized_search_cv.best_score_)\n",
    "\n",
    "predictions = best_random.predict(testing_features)\n",
    "mae = metrics.mean_absolute_error(testing_target, predictions)\n",
    "mse = metrics.mean_squared_error(testing_target, predictions)\n",
    "rmse = mse**0.5\n",
    "mse = metrics.mean_squared_error(testing_target, predictions)\n",
    "r2 = metrics.r2_score(testing_target, predictions)\n",
    "print(\"--------------------------------\")\n",
    "print(rmse)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## XGBoost with G=grid search to locate the scale of best hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 2000, num = 20)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "_random_state = 42\n",
    "xgb_regressor = XGBRegressor(random_state=_random_state)\n",
    "\n",
    "Grid_Search_cv = model_selection.GridSearchCV(\n",
    "    xgb_regressor,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "Grid_Search_cv.fit(training_features, training_target)\n",
    "best_grid = Grid_Search_cv.best_estimator_\n",
    "gsbs = Grid_Search_cv.best_params_\n",
    "print(\"--------------------------------\")\n",
    "print(\"Model Best parameters.........\\n\")\n",
    "\n",
    "print(gsbs)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# RMSE for the best parameters\n",
    "print(-Grid_Search_cv.best_score_)\n",
    "\n",
    "predictions = best_grid.predict(testing_features)\n",
    "mae = metrics.mean_absolute_error(testing_target, predictions)\n",
    "mse = metrics.mean_squared_error(testing_target, predictions)\n",
    "rmse = mse**0.5\n",
    "mse = metrics.mean_squared_error(testing_target, predictions)\n",
    "r2 = metrics.r2_score(testing_target, predictions)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"--------Test the model on hideout test data----------\\n\")\n",
    "print(\"The RMSE score for the test :\", rmse)\n",
    "print(\"The R^2 score for the test :\", r2)\n",
    "print(\"The RMSE score for the test :\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(testing_target, predictions)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutF = open(\"C:\\\\Users\\\\Honeyz\\\\Desktop\\\\Modeling\\\\Final models\\\\CoPyro Project\\\\Results\\\\Performace_Output_Char_RF_Notebook.txt\",\"a\")\n",
    "OutF.write(\"\\n RMSE: \")\n",
    "OutF.write(str(round(rmse,3)))\n",
    "OutF.write(\"\\n MAE: \")\n",
    "OutF.write(str(round(mae,3)))\n",
    "OutF.write(\"\\n RSBS: \")\n",
    "OutF.write(str(rsbs))\n",
    "OutF.write(\"\\n\")\n",
    "OutF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutTD = open(\"C:\\\\Users\\\\Honeyz\\\\Desktop\\\\Modeling\\\\Final models\\\\CoPyro Project\\\\Results\\\\TestData_Output_Char_RF_Notebook.txt\",\"a\")\n",
    "OutTD.write(\"\\n Actual data: \")\n",
    "OutTD.write(str(testing_target.values))\n",
    "OutTD.write(\"\\n Prediction data: \")\n",
    "OutTD.write(str(predictions))\n",
    "OutTD.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename = 'C:\\\\Users\\\\Honeyz\\\\Desktop\\\\Modeling\\\\Final models\\\\CoPyro Project\\\\Saved Models\\\\RF_Char_Model_Proximate_3.sav'\n",
    "pickle.dump(regressor, open(filename, 'wb'))\n",
    "\n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)"
   ]
  }
 ]
}